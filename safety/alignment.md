---
permalink: /safety/alignment/
---
# Value Alignment

The motivation to study AI alignment ([Yudkowsky, 2016](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/)) is to build AI that behaves ethically so that smarter-than-human intelligence has a positive outcome ([Soares, 2017](https://intelligence.org/2017/04/12/ensuring/)). Ethical AI is still an open question ([Pavaloiu & Kose, 2017](https://arxiv.org/abs/1706.03021)).

[Lawrence (2017)](https://arxiv.org/abs/1705.07996) argues that we've already developed non-sentient, reactive machine intelligence that is aligned with our subconscious desires, but not our aspirations. Its effects, such as fake news, can confirm our biases and prejudice, undermine our spirit and soul.

* 2017 May 22, Neil D. Lawrence. [Living Together: Mind and Machine Intelligence](https://arxiv.org/abs/1705.07996). *arXiv:1705.07996*.
* 2017 May 16, Alice Pavaloiu and Utku Kose. [Ethical Artificial Intelligence - An Open Question](https://arxiv.org/abs/1706.03021). *arXiv:1706.03021*.
* 2017 April 12, Nate Soares. [Ensuring smarter-than-human intelligence has a positive outcome](https://intelligence.org/2017/04/12/ensuring/). *Machine Intelligence Research Institute*.
* 2016 December 28, Eliezer Yudkowsky. [AI Alignment: Why Itâ€™s Hard, and Where to Start](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/). *Machine Intelligence Research Institute*.

## Reward Learning

For a sophisticated reinforcement learning agent to act in a way aligned with human values, it needs to learn an appropriate reward function. [Christiano et al. (2017)](https://arxiv.org/abs/1706.03741) showed that neural network-based reward predictors can be learned efficiently from non-expert human preferences between pairs of trajectory segments.

### References

* 2017 June 12, Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. [Deep reinforcement learning from human preferences](https://arxiv.org/abs/1706.03741). *arXiv:1706.03741*. [code](https://github.com/nottombrown/rl-teacher).

